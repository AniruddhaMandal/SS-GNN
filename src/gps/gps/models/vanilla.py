import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import (
    GINEConv, GINConv, GCNConv, SAGEConv, GATv2Conv, global_mean_pool, global_add_pool, global_max_pool
)
from torch_geometric.nn.norm import BatchNorm
from gps import SubgraphFeaturesBatch


def make_mlp(in_dim, hidden_dim, out_dim, num_layers=2, activate_last=False):
    layers = []
    if num_layers == 1:
        layers += [nn.Linear(in_dim, out_dim)]
        if activate_last: layers += [nn.ReLU()]
    else:
        layers += [nn.Linear(in_dim, hidden_dim), nn.ReLU()]
        for _ in range(num_layers - 2):
            layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU()]
        layers += [nn.Linear(hidden_dim, out_dim)]
        if activate_last: layers += [nn.ReLU()]
    return nn.Sequential(*layers)


class VanillaGNNClassifier(nn.Module):
    """
    A flexible graph-level classifier:
      - conv_type: 'gine' (uses edge_attr), 'gin', 'gcn', 'sage', 'gatv2'
      - For Peptides-func (multi-label), uses BCE-with-logits style outputs.
    """
    def __init__(self,
                 in_channels: int,
                 edge_dim: int,
                 hidden_dim: int,
                 out_dim: int = 10,
                 num_layers: int = 5,
                 mlp_layers: int = 2,
                 dropout: float = 0.1,
                 conv_type: str = 'gine',
                 pooling: str = 'mean'):
                 
        super().__init__()
        conv_type = conv_type.lower()
        assert conv_type in {'gine', 'gin', 'gcn', 'sage', 'gatv2'}
        assert pooling in {'mean', 'add', 'max', 'sum', 'off'}
        
        self.conv_type = conv_type
        self.num_layers = num_layers
        self.dropout = dropout
        self.out_dim = out_dim
        self.pooling = pooling

        # Project inputs to hidden_dim once so all layers are width-matched.
        self.node_proj = nn.Linear(in_channels, hidden_dim)

        # If using edge features (only GINE uses them directly)
        self.use_edges = (conv_type == 'gine')
        if self.use_edges:
            self.edge_proj = nn.Linear(edge_dim, hidden_dim)

        # Build layers
        self.convs = nn.ModuleList()
        self.bns   = nn.ModuleList()
        for layer in range(num_layers):
            self.convs.append(self._make_conv(hidden_dim, hidden_dim, mlp_layers))
            self.bns.append(BatchNorm(hidden_dim))

        if self.pooling == 'mean':
            self.pooling_fn = global_mean_pool
        elif self.pooling in ['add', 'sum']:
            self.pooling_fn = global_add_pool
        elif self.pooling == 'max':
            self.pooling_fn = global_max_pool
        elif self.pooling == 'off':
            self.pooling_fn = lambda h, b: h
    
    def _make_conv(self, in_dim, out_dim, mlp_layers):
        if self.conv_type == 'gine':
            mlp = make_mlp(in_dim, in_dim, out_dim, num_layers=mlp_layers)
            return GINEConv(nn=mlp, train_eps=True)
        if self.conv_type == 'gin':
            mlp = make_mlp(in_dim, in_dim, out_dim, num_layers=mlp_layers)
            return GINConv(nn=mlp, train_eps=True)
        if self.conv_type == 'gcn':
            return GCNConv(in_dim, out_dim, cached=False, normalize=True)
        if self.conv_type == 'sage':
            return SAGEConv(in_dim, out_dim)
        if self.conv_type == 'gatv2':
            return GATv2Conv(in_dim, out_dim, heads=1, concat=True)
        raise ValueError("Unknown conv_type")

    # ---------- FORWARD ----------
    def forward(self, batch: SubgraphFeaturesBatch):
        """
        x: [N, in_channels]
        edge_index: [2, E]
        batch: [N]
        edge_attr: [E, edge_dim]  (required if conv_type='gine')
        """
        x = batch.x
        edge_attr = batch.edge_attr
        edge_index = batch.edge_index
        
        h = self.node_proj(x)
        if self.use_edges:
            if edge_attr is None:
                raise ValueError("edge_attr is required for conv_type='gine'.")
            e = self.edge_proj(edge_attr)
        
        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):
            h_res = h
            h = conv(h, edge_index) if not self.use_edges else conv(h, edge_index, e)
            h = bn(h)
            h = F.relu(h)
            h = h + h_res  
            if i < self.num_layers - 1:  # Skip dropout on last layer
                h = F.dropout(h, p=self.dropout, training=self.training)
        
        g = self.pooling_fn(h, batch.batch)
        return g