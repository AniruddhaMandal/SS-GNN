{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d12d23-978c-4159-8849-e786bcea562d",
   "metadata": {},
   "source": [
    "# Comparision of Weisfeiler Lehman and SS-GNN \n",
    "## Observations:\n",
    "* The SS-GNN cannot be more expressive than probabilistic SS-WL and SS-WL is more expressive than probabilistic SS-WL\n",
    "* SS-GNN encodings for two graphs with similar WL-hash should be same. Atleast the distance between the encodings should be same.\n",
    "\n",
    "## Experiment: \n",
    "* Extract the activations from ss_gnn activation.\n",
    "* The `conv_i` contains the encoding for each node.\n",
    "* After the pooling layer we get the encodings for each subgraph in the batch.\n",
    "* The subgraphs with same wl encoding should have the ss-gnn encodings in an $\\epsilon$-ball, for some $\\epsilon>0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b04e6f6-13f8-4749-a79e-9c04182124ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from gps.utils.data_transform import SetNodeFeaturesOnes\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from uniform_sampler import sample_batch as sampler\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gps.models.ss_gnn import SubgraphGNNEncoder, SubgraphSamplingGNNClassifier\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.conv import GCNConv, GINConv\n",
    "from torch_geometric.nn.pool import global_add_pool\n",
    "import torch_geometric.nn as pyg\n",
    "\n",
    "from torch_geometric.utils import scatter\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596a8254-2395-47c2-a1fb-a748e01c8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseSubgraphEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_layers, pooling='sum', residual=False,batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "        self.proj = nn.Linear(in_dim,hidden_dim,bias=False)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            neural_net = self._make_neural_net(in_dim=hidden_dim, out_dim=hidden_dim)\n",
    "            self.convs.append(GINConv(neural_net))\n",
    "            if batch_norm:\n",
    "                self.bns.append(pyg.BatchNorm(hidden_dim))\n",
    "        if pooling == 'sum':\n",
    "            self.pooling = pyg.global_add_pool\n",
    "        self.activations = {}\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.proj(x)\n",
    "        self.activations['proj_0'] = h.clone()\n",
    "        for l in range(self.num_layers):\n",
    "            h_res = h\n",
    "            h = self.convs[l](h,edge_index)\n",
    "            self.activations[f'conv_{l}'] = h\n",
    "            if self.batch_norm:\n",
    "                h = self.bns[l](h)\n",
    "                self.activations[f'bns_{l}'] = h\n",
    "            if self.residual:\n",
    "                h = h + h_res\n",
    "                self.activations[f'res_{l}'] = h\n",
    "        h_out = self.pooling(h,batch)\n",
    "        self.activations['pooling'] = h_out\n",
    "        return h_out\n",
    "        \n",
    "    def _make_neural_net(self, in_dim, out_dim, num_layers=2):\n",
    "        layers = nn.ModuleList()\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        for _ in range(num_layers-1):\n",
    "            layers.append(nn.Linear(out_dim, out_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        return nn.Sequential(*layers)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5250ff17-d2c5-4eab-b145-bafbc038ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseAttentionAggregator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_dim, \n",
    "                 temperature=0.2, \n",
    "                 pooling='sum'):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.attention_mlp = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, hidden_dim//2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim//2,1)\n",
    "            )\n",
    "        self.activations = {}\n",
    "        \n",
    "    def forward(self, subgraph_embeddings, batch):\n",
    "        num_graphs = batch.max().item()+1\n",
    "        scores = self.attention_mlp(subgraph_embeddings)\n",
    "        scores = scores/self.temperature\n",
    "\n",
    "        # find the max score along all subgraphs of each graph of the batch\n",
    "        max_scores = scatter(scores, batch, dim=0, dim_size=num_graphs, reduce='max')\n",
    "        # assign the corresponding max score to each subgraph of the batch\n",
    "        max_scores = max_scores[batch]\n",
    "        # e^{-(max_scores-scores)}\n",
    "        scores_exp = torch.exp(scores-max_scores)\n",
    "        # sum exp scores along each graph\n",
    "        scores_sum = scatter(scores_exp, batch, dim=0, dim_size=num_graphs, reduce='sum')\n",
    "        # boadcast socres_sums along each subgraph\n",
    "        scores_sum = scores_sum[batch]\n",
    "        # normalizing the scores_exp\n",
    "        attention_weights = scores_exp / (scores_sum + 1e-8)\n",
    "        \n",
    "        '''scaling subgraph embeddings with sttention_weights\n",
    "            attention_weights should prioritize significant subgraphs\n",
    "            and scalling with attention weight should enhance (increase\n",
    "            in magnitude) the important subgraph embeddings. \n",
    "        '''\n",
    "        self.activations['attention_weights'] = attention_weights\n",
    "        weighted_embeddings = attention_weights * subgraph_embeddings\n",
    "        # finally, weighted sum for all subgraphs of each graph\n",
    "        graphs_embeddings = scatter(weighted_embeddings, batch, dim=0, dim_size=num_graphs, reduce='sum')\n",
    "        return graphs_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a39ebd6-c2c6-4920-b0b7-1cadc20de280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create ss-gnn input from dataset batch\n",
    "def build_input(batch, k, m):\n",
    "      num_subgraphs = (batch.batch.max()+1)*m\n",
    "      nodes_sampled, edge_index_sampled, edge_ptr, sample_ptr, edge_src_global = \\\n",
    "          sampler(batch.edge_index, batch.ptr, m_per_graph=m, k=k)\n",
    "      x_global = batch.x[nodes_sampled.flatten()]\n",
    "      edge_index_global = torch.repeat_interleave(torch.arange(0,num_subgraphs), edge_ptr[1:]-edge_ptr[:-1])*k + edge_index_sampled  \n",
    "      sample_id = torch.repeat_interleave(torch.arange(0,num_subgraphs), k)\n",
    "      return x_global, edge_index_global, sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5045ae39-1127-438a-b8a1-c774a5fc1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = 5\n",
    "hidden_dim = 64\n",
    "k = 6\n",
    "m = 100\n",
    "\n",
    "# data\n",
    "transform = SetNodeFeaturesOnes(dim=data_dim)\n",
    "data = GNNBenchmarkDataset(\"./.temp/CSL\", name='CSL', transform=transform)\n",
    "loader = DataLoader(data,batch_size=15, shuffle=False)\n",
    "\n",
    "# model\n",
    "verbose_encoder = VerboseSubgraphEncoder(in_dim=5,hidden_dim=10, num_layers=4)\n",
    "verbose_attention = VerboseAttentionAggregator(hidden_dim=10)\n",
    "for i, batch in enumerate(loader):\n",
    "    y = batch.y\n",
    "    num_graphs = batch.batch.max()+1\n",
    "    \n",
    "    x_global, edge_index_global, sample_id = build_input(batch, k, m)\n",
    "    \n",
    "    subgraph_encodings = verbose_encoder(x_global, edge_index_global, sample_id)\n",
    "    \n",
    "    graph_id = torch.repeat_interleave(torch.arange(num_graphs),m)\n",
    "    graph_encodings = verbose_attention(subgraph_encodings,graph_id)\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "enc_activations = verbose_encoder.activations\n",
    "agg_activations = verbose_attention.activations\n",
    "subgraph_enc = subgraph_encodings.detach()\n",
    "graph_enc = graph_encodings.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a30e2ea-88fe-4bc5-97c7-2efd0f3216ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def wl_graph_hash_batch(edge_index, batch, node_features=None, num_iterations=3):\n",
    "    \"\"\"\n",
    "    Compute WL hash for batched graphs.\n",
    "    \n",
    "    Args:\n",
    "        edge_index: (2, num_edges) - edge indices\n",
    "        batch: (num_nodes,) - batch assignment for each node\n",
    "        node_features: (num_nodes, feature_dim) or None\n",
    "        num_iterations: number of WL iterations\n",
    "    \n",
    "    Returns:\n",
    "        hashes: list of hash strings, one per graph\n",
    "    \"\"\"\n",
    "    num_nodes = batch.size(0)\n",
    "    num_graphs = batch.max().item() + 1\n",
    "    \n",
    "    # Initialize labels\n",
    "    if node_features is None:\n",
    "        labels = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    else:\n",
    "        labels = torch.tensor([hash(tuple(f.tolist())) % (2**31) \n",
    "                               for f in node_features], dtype=torch.long)\n",
    "    \n",
    "    # WL iterations\n",
    "    for _ in range(num_iterations):\n",
    "        new_labels = torch.zeros(num_nodes, dtype=torch.long)\n",
    "        \n",
    "        # Build adjacency for efficient neighbor lookup\n",
    "        adj_dict = {i: [] for i in range(num_nodes)}\n",
    "        for src, tgt in edge_index.t().tolist():\n",
    "            adj_dict[src].append(tgt)\n",
    "        \n",
    "        for node in range(num_nodes):\n",
    "            neighbors = adj_dict[node]\n",
    "            if neighbors:\n",
    "                neighbor_labels = labels[neighbors].sort()[0].tolist()\n",
    "            else:\n",
    "                neighbor_labels = []\n",
    "            \n",
    "            signature = str([labels[node].item()] + neighbor_labels)\n",
    "            new_labels[node] = int(hashlib.md5(signature.encode()).hexdigest(), 16) % (2**31)\n",
    "        \n",
    "        labels = new_labels\n",
    "    \n",
    "    # Aggregate to graph-level hashes\n",
    "    graph_hashes = []\n",
    "    for g in range(num_graphs):\n",
    "        mask = batch == g\n",
    "        graph_labels = sorted(labels[mask].tolist())\n",
    "        graph_sig = str(graph_labels)\n",
    "        graph_hash = hashlib.md5(graph_sig.encode()).hexdigest()\n",
    "        graph_hashes.append(graph_hash)\n",
    "    \n",
    "    return graph_hashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3702f144-a707-4956-b9e7-8cbb45da3079",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def unique_with_epsilon(tensor, epsilon=1e-6, return_counts=False):\n",
    "    \"\"\"\n",
    "    Find unique values in a tensor where values within epsilon are considered the same.\n",
    "    \n",
    "    Args:\n",
    "        tensor: 1D PyTorch tensor\n",
    "        epsilon: tolerance for considering values as equal\n",
    "        return_counts: if True, also return counts for each unique value\n",
    "    \n",
    "    Returns:\n",
    "        unique_values: Tensor of unique values\n",
    "        counts (optional): Tensor of counts for each unique value\n",
    "    \"\"\"\n",
    "    if len(tensor) == 0:\n",
    "        if return_counts:\n",
    "            return tensor, torch.tensor([], dtype=torch.long)\n",
    "        return tensor\n",
    "    \n",
    "    # Sort the tensor\n",
    "    sorted_tensor, _ = torch.sort(tensor)\n",
    "    \n",
    "    # Find where consecutive differences are greater than epsilon\n",
    "    diffs = torch.diff(sorted_tensor)\n",
    "    mask = torch.cat([torch.tensor([True]), diffs > epsilon])\n",
    "    \n",
    "    # Select unique values\n",
    "    unique_values = sorted_tensor[mask]\n",
    "    \n",
    "    if return_counts:\n",
    "        # Find cluster boundaries\n",
    "        boundaries = torch.cat([torch.tensor([True]), diffs > epsilon, torch.tensor([True])])\n",
    "        boundary_indices = torch.where(boundaries)[0]\n",
    "        \n",
    "        # Compute counts for each cluster\n",
    "        counts = torch.diff(boundary_indices)\n",
    "        \n",
    "        return unique_values, counts\n",
    "    \n",
    "    return unique_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d029cc-bd8c-41bf-8300-a8308d12501d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wl_graph_hash_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m hashs = \u001b[43mwl_graph_hash_batch\u001b[49m(edge_index_global, sample_id, x_global, num_iterations=\u001b[32m1\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mWL-Distinct subgraphs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.unique(hashs).\u001b[34m__len__\u001b[39m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of subgraphss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgraph_enc.\u001b[34m__len__\u001b[39m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'wl_graph_hash_batch' is not defined"
     ]
    }
   ],
   "source": [
    "hashs = wl_graph_hash_batch(edge_index_global, sample_id, x_global, num_iterations=1)\n",
    "print(f'WL-Distinct subgraphs: {np.unique(hashs).__len__()}')\n",
    "print(f\"Number of subgraphss: {subgraph_enc.__len__()}\")\n",
    "\n",
    "subgraph_enc_dist = torch.cdist(subgraph_enc,subgraph_enc)\n",
    "print(\"We need to see the unique encodings in list of subgraph encodings!\")\n",
    "print(f\"Unique distances from the first subgraphs to other graph: {torch.unique(subgraph_enc_dist[0]).__len__()}\")\n",
    "print(f\"Number of unique subgraph encodings with small numerical tollaracne: {unique_with_epsilon(subgraph_enc_dist[0], epsilon=1e-3).__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5173a6-3cc1-4bd8-9cc0-8a311f39838c",
   "metadata": {},
   "source": [
    "*Now compare the ss-gnn encoder output with `hashs`. Observe the encoding distances between graphs with same WL hash.*\n",
    "* If graphs withs same WL encoding has encoding distant high then something is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd04463-47e1-4e6e-a6f2-01d8b346ec58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Equivalance of PSS-WL and SS-GNN: \n",
    "**8 PSS-WL distinguishable subgraphs in sample of the first graph.**<br>\n",
    "**Similarly, 8 SS-GNN distinguishable subgraph present (with neumerical tollarace $1e-3$).**<br>\n",
    "Now we observe the whether frequences of each class to identify the whether the emperical distributions are also same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa1f128c-9f99-4860-aa37-33dd8a64adfd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash frequencies in the first: [ 55  46 599  87 283 189 188  53]\n",
      "unique subgraph frequencies: tensor([599, 283, 189,  53, 188,  55,  87,  46])\n"
     ]
    }
   ],
   "source": [
    "unique_hash, hash_feq = np.unique(hashs,return_counts=True)\n",
    "print(f\"Hash frequencies in the first: {hash_feq}\")\n",
    "unique_subgraphs, subgraph_feq = unique_with_epsilon(subgraph_enc_dist[0], epsilon=1e-4, return_counts=True)\n",
    "print(f\"unique subgraph frequencies: {subgraph_feq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c530ba1e-bac9-432d-972f-66be1df6792a",
   "metadata": {},
   "source": [
    "So the emperical distributions are also same. So, the model is **working upto SubgraphGNNEncoder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1751c68-ccd4-41e4-bbef-274974dacb27",
   "metadata": {},
   "source": [
    "# After aggregation:\n",
    "* After aggregation the graphs with same lebel(10-graphs) have slightly different encodigns.\n",
    "* We need to find out whether the encodings of SS_GNN for other graphs with different labels are significantly different\n",
    "* If they are not significantly different, the attention might learn to distinguish these graphs during training\n",
    "  Now we study unique distances from first graph. As the class is same the distances should be small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8caa55c4-e5c9-4c3b-b5e4-6bcff822303e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0057, 0.0077, 0.0094, 0.0096, 0.0132, 0.0145, 0.0146, 0.0265,\n",
       "        0.0292, 0.0303, 0.0337, 0.0371, 0.0438, 0.0663])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(torch.cdist(graph_enc, graph_enc)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24c221-8364-42e1-93a9-360e405c29f1",
   "metadata": {},
   "source": [
    "So we see the distances are small enough. Now see how the graph embeddings for two graphs of different labels differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71cdb23-508f-4403-bd30-35d462566038",
   "metadata": {},
   "source": [
    "### for second batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05c42dae-7cda-447d-875b-39e2763d48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = 5\n",
    "hidden_dim = 64\n",
    "k = 6\n",
    "m = 100\n",
    "\n",
    "# data\n",
    "transform = SetNodeFeaturesOnes(dim=data_dim)\n",
    "data = GNNBenchmarkDataset(\"./.temp/CSL\", name='CSL', transform=transform)\n",
    "loader = DataLoader(data,batch_size=15, shuffle=False)\n",
    "\n",
    "# model\n",
    "verbose_encoder = VerboseSubgraphEncoder(in_dim=5,hidden_dim=10, num_layers=4)\n",
    "verbose_attention = VerboseAttentionAggregator(hidden_dim=10)\n",
    "for i, batch in enumerate(loader):\n",
    "    y = batch.y\n",
    "    num_graphs = batch.batch.max()+1\n",
    "    \n",
    "    x_global, edge_index_global, sample_id = build_input(batch, k, m)\n",
    "    \n",
    "    subgraph_encodings = verbose_encoder(x_global, edge_index_global, sample_id)\n",
    "    \n",
    "    graph_id = torch.repeat_interleave(torch.arange(num_graphs),m)\n",
    "    graph_encodings = verbose_attention(subgraph_encodings,graph_id)\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "enc_activations = verbose_encoder.activations\n",
    "agg_activations = verbose_attention.activations\n",
    "subgraph_enc = subgraph_encodings.detach()\n",
    "graph_enc_1 = graph_encodings.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "217f948d-6600-4002-927f-e218b08b403e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.2890, 3.2918, 3.2909, 3.2927, 3.2905, 3.2906, 3.2906, 3.2917, 3.2920,\n",
       "         3.2884, 3.2903, 3.2918, 3.2917, 3.2926, 3.2914],\n",
       "        [3.3335, 3.3364, 3.3355, 3.3373, 3.3350, 3.3351, 3.3351, 3.3362, 3.3366,\n",
       "         3.3329, 3.3349, 3.3364, 3.3362, 3.3372, 3.3360],\n",
       "        [3.3223, 3.3251, 3.3242, 3.3260, 3.3238, 3.3239, 3.3238, 3.3250, 3.3253,\n",
       "         3.3216, 3.3236, 3.3251, 3.3249, 3.3259, 3.3247],\n",
       "        [3.3199, 3.3228, 3.3218, 3.3237, 3.3214, 3.3215, 3.3215, 3.3226, 3.3229,\n",
       "         3.3193, 3.3212, 3.3227, 3.3226, 3.3236, 3.3223],\n",
       "        [3.2937, 3.2966, 3.2956, 3.2975, 3.2952, 3.2953, 3.2953, 3.2964, 3.2967,\n",
       "         3.2931, 3.2950, 3.2965, 3.2964, 3.2973, 3.2961],\n",
       "        [3.3029, 3.3058, 3.3048, 3.3066, 3.3044, 3.3045, 3.3045, 3.3056, 3.3059,\n",
       "         3.3023, 3.3042, 3.3057, 3.3056, 3.3065, 3.3053],\n",
       "        [3.3037, 3.3065, 3.3056, 3.3074, 3.3052, 3.3053, 3.3052, 3.3064, 3.3067,\n",
       "         3.3031, 3.3050, 3.3065, 3.3063, 3.3073, 3.3061],\n",
       "        [3.3027, 3.3056, 3.3047, 3.3065, 3.3042, 3.3043, 3.3043, 3.3055, 3.3058,\n",
       "         3.3021, 3.3041, 3.3055, 3.3054, 3.3064, 3.3051],\n",
       "        [3.3188, 3.3217, 3.3208, 3.3226, 3.3203, 3.3204, 3.3204, 3.3215, 3.3219,\n",
       "         3.3182, 3.3201, 3.3216, 3.3215, 3.3225, 3.3212],\n",
       "        [3.2719, 3.2747, 3.2738, 3.2756, 3.2734, 3.2735, 3.2735, 3.2746, 3.2749,\n",
       "         3.2713, 3.2732, 3.2747, 3.2746, 3.2755, 3.2743],\n",
       "        [3.3162, 3.3190, 3.3181, 3.3199, 3.3177, 3.3178, 3.3178, 3.3189, 3.3192,\n",
       "         3.3155, 3.3175, 3.3190, 3.3188, 3.3198, 3.3186],\n",
       "        [3.3269, 3.3298, 3.3288, 3.3306, 3.3284, 3.3285, 3.3285, 3.3296, 3.3299,\n",
       "         3.3263, 3.3282, 3.3297, 3.3296, 3.3306, 3.3293],\n",
       "        [3.2960, 3.2989, 3.2979, 3.2998, 3.2975, 3.2976, 3.2976, 3.2987, 3.2990,\n",
       "         3.2954, 3.2973, 3.2988, 3.2987, 3.2996, 3.2984],\n",
       "        [3.3186, 3.3215, 3.3206, 3.3224, 3.3202, 3.3202, 3.3202, 3.3214, 3.3217,\n",
       "         3.3180, 3.3200, 3.3215, 3.3213, 3.3223, 3.3211],\n",
       "        [3.3204, 3.3233, 3.3224, 3.3242, 3.3220, 3.3220, 3.3220, 3.3232, 3.3235,\n",
       "         3.3198, 3.3218, 3.3233, 3.3231, 3.3241, 3.3229]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(graph_enc, graph_enc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79f0ed-bda6-460d-ad8c-ec3a7ed21cc6",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "* The graph embeddings for graphs of label 0 and label 1 have significant difference\n",
    "* **Thus SS-GNN can distinguish distinct CSL graphs** Success!!!ðŸ˜ŽðŸ˜ŽðŸ˜Ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb3e56-8a1d-4ffb-896e-8cc5b87b27f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
